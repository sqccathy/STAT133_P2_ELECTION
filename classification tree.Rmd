---
title: "rpart"
output: html_document
---
First we load the dataframe we get in first part.
```{r}
load("finalDataFrame.rda")
```

We add one column in the dataframe that indicate which party was mostly voted by the county. Then we subset those dataframe,removing the names of the county and the election result in previous years. We want to use census data to predict the election results for 2016.
```{r}
final$res2016 = final$ClintonVote2016>final$TrumpVote2016
states = unique(final$State)
set.seed(12345678)
chooseTrain = unlist(lapply(states, function(x) {
  Counties = which(final$State == x)
  n = length(Counties)
  if(n > 1)
  sample = sample(Counties, ceiling(n / 2))
  else sample = Counties
  return(sample)
}))
chooseTrain = chooseTrain[seq(1500)]
final = final[,-seq(1:12)]
```

We split the data in to test part and train part.
```{r}
finalTest = final[-chooseTrain, ]
finalTrain = final[chooseTrain, ]
```

We want to use the cross-validation to train our model. We decide to split them into three folds.
```{r}
set.seed(12344321)
permuteIndices = sample(length(chooseTrain))
v = 3
folds = matrix(permuteIndices,ncol = 3)
```

We want to find the cp that will produce the highest precision. So we get a vector of cp and use for loop to try them out.
```{r}
cps = c(seq(0.0001, 0.001, by = 0.0001), 
       seq(0.001, 0.01, by = 0.001),
       seq(0.01, 0.1, by = 0.01))

preds = matrix(nrow = length(chooseTrain), ncol = length(cps))
library(rpart)
for (i in 1:v) {
  trainFold = as.integer(folds[, -i])
  testFold = folds[, i]
  
  for (j in 1:length(cps)) {
    tree = rpart(res2016 ~ .,
            data = finalTrain[trainFold,],
            method = "class",
            control = rpart.control(cp = cps[j]))
    preds[testFold,j] =
      predict(tree,
              newdata = finalTrain[testFold,-length(testFold)],
              type = "class")
  }
}
```

After the cross-validation, we get the prediction result for the train set and by converting them into false and true, we are able to compare it with the true result and get the accuracy rate.
```{r}
cvRates = apply(preds, 2, function(oneSet) {
  oneSet[which(oneSet == 1)] = "FALSE"
  oneSet[which(oneSet == 2)] = "TRUE"
  sum(finalTrain$res2016 == oneSet)/length(chooseTrain)
})
```

We make a plot to see which cp will produce the highest precision.It is going up for a while and going down after reaching it peak.
```{r}
library(ggplot2)
cvRes = data.frame(cps, cvRates)
ggplot(data = cvRes, aes(x = cps, y = cvRates)) +
  geom_line() + 
  labs(x = "Complexity Parameter", y = "Classification Rate")
```

We choose a best cp to build our final predictor. We train the model by randomly subset the whole dataframe and predict the result of the whole dataframe. Its accuracy rate is 0.92,which is behaving pretty well.
```{r}
cpChoice = cvRes[which.max(cvRates)-6,1]
# rand = sample(1:3104,2000)
# finaltraindf = final[rand,]
finalTree = rpart(res2016 ~ .,
                  data = finalTrain,
                  method = "class",
                  control = rpart.control(cp = cpChoice))
testPreds = predict(finalTree, 
              newdata = finalTest[,-31],
              type = "class")

classRate = sum(testPreds == finalTest$res2016) / nrow(finalTest)

classRate
```

We draw the tree to see how many levels do we have and which variables are used to predict.
```{r}
library(rpart.plot)
prp(finalTree, extra = 2)
```

We want to figure out when the model makes mistakes. So we add the predict result into the dataframe as a column. There are four cases, which is predict true when it is true, predict false when it is true, predict false when it is false and predict true when it is false.
```{r}
truetrue = finalTest[which(testPreds==TRUE&finalTest$res2016==TRUE),]
falsefalse = finalTest[which(testPreds==FALSE&finalTest$res2016==FALSE),]
truefalse = finalTest[which(testPreds==TRUE&finalTest$res2016==FALSE),]
falsetrue = finalTest[which(testPreds==FALSE&finalTest$res2016==TRUE),]

truetrue$pred = "righttrue"
falsefalse$pred = "rightfalse"
truefalse$pred = "wrongtrue"
falsetrue$pred = "wrongfalse"

predfinal = rbind(truetrue,falsefalse,truefalse,falsetrue)
```

A type I error is the incorrect rejection of a true null hypothesis (a "false positive"), while a type II error is incorrectly retaining a false null hypothesis (a "false negative").It turns out that the type I error is 0.07, the type II error is 0.02
```{r}
type1error = nrow(falsetrue)/nrow(predfinal)
type2error = nrow(truefalse)/nrow(predfinal)
```

From the tree I built above, the node femaleUnemployment doesn't have enough purity. It shows that false has 10/15 and true has 17/21. So I think the wrong predection may happen here. I make a plot to verify my guess. 
```{r}
ggplot(predfinal)+geom_density(mapping = aes(x = femaleUnemployment, color = pred))
```

It turns out that there are not much difference in femaleUnemployment between voting for Democratic and Republic. It means the femaleUnemployment and votes are not closely related. So when we make prediction based on femaleUnemployment, chances are that the predictor will make mistakes.


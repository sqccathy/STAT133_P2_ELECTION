---
title: "KNN"
author: "Jieni Wan"
date: "2016"
output: html_document
---
### Predicting the Change from 2012 to 2016

  We use the k nearest method to build a predictor for the election result change from 2012 to 2016, i.e., we'll predict whether a county will stay voting for the Democratic/Republican party, or it will swing from Democratic to Republican, or the other way around. To meet this end, we will be using the k nearest neighbor algorithm.

#### Data Frame Manipulation

  First, we will generate the observations for the change. We call this variable "change1216", and add it to our data frame.
```{r echo = codeInclude}
final$change1216 = character(nrow(final))

final$change1216[(final$ObamaVote2012 > final$RomneyVote2012) & (final$ClintonVote2016 > final$TrumpVote2016)] = "DD"

final$change1216[(final$ObamaVote2012 > final$RomneyVote2012) & (final$ClintonVote2016 < final$TrumpVote2016)] = "DR"

final$change1216[(final$ObamaVote2012 < final$RomneyVote2012) & (final$ClintonVote2016 > final$TrumpVote2016)] = "RD"

final$change1216[(final$ObamaVote2012 < final$RomneyVote2012) & (final$ClintonVote2016 < final$TrumpVote2016)] = "RR"

final$change1216 = as.factor(final$change1216)
```

  By saying neighbors of a particular county, we mean that they share similar characteristics with this county. These charateristics, not only including geographic information, but also other features concerning family, economy and population. The definition of neighbors in this case is based on our expectation that counties sharing alike features are more likely to behave similarly. According to the findings in Step 2, here we choose 6 features which we assume have significant influence upon the results, namely percent of white population, percent of the civilian labor force employed in professional, scientific, management, administrative and waste management services industries, percent of single mothers, percent of unemployment, percent of unemployed women and percent of never married men. 
  
  Also, we add some other features that we assume may have influence on the results. To figure out which features are more influential, we change the combination of features and compute the error rates respectively.
  
  Similarly, we include state information in the new data frames.
```{r echo = codeInclude}
final1 = final[c("State", "Latitude", "Longitude", "maleHouseholder", "livingAlone", "percentWhite", "femaleUnemployment", "neverMarriedMen", "singleMoms", "professionalIndustries", "unemployed")]

final2 = final[c("State", "Latitude", "Longitude", "marriedcouples", "familiesWithKids", "percentWhite", "femaleUnemployment", "neverMarriedMen", "singleMoms", "professionalIndustries", "unemployed")]

final3 = final[c("State", "Latitude", "Longitude", "constructionIndustry", "avgFamilySize", "percentWhite", "femaleUnemployment", "neverMarriedMen", "singleMoms", "professionalIndustries", "unemployed")]
```

  Also, we find that the latitude and longitude are of the class character. Thus we turn them into numerics, for otherwise we cannot calculate the distances between counties based on geographic information.
```{r}
final1$Latitude = as.numeric(final1$Latitude)
final1$Longitude = as.numeric(final1$Longitude)

final2$Latitude = as.numeric(final2$Latitude)
final2$Longitude = as.numeric(final2$Longitude)

final3$Latitude = as.numeric(final3$Latitude)
final3$Longitude = as.numeric(final3$Longitude)
```
  
  Note that here we need a distance matrix, meaning the features that decide the distances have to be of the same scale. Therefore we normalize these variables.
```{r}
final1[-1] = sapply(final1[-1], function(x) (x - mean(x)) / sd(x))

final2[-1] = sapply(final2[-1], function(x) (x - mean(x)) / sd(x))

final3[-1] = sapply(final3[-1], function(x) (x - mean(x)) / sd(x))
``` 


#### Train and Test Data

  As we discussed before, we want to "randomly" place some counties in the test set, but we also want to make sure that each state has enough counties in the train set. For each state, we find all the counties in it, and then sample these rows for the train set. After this step we can drop the state variable.
  
  Notice that we do not need to sample rows for the train set again. Since we are making comparisons, we just need to change the features and keep the other factors the same, including the train and test sets. 
```{r echo = codeInclude}
states = unique(final1$State)
set.seed(12345678)
chooseTrain = unlist(lapply(states, function(x) {
  Counties = which(final1$State == x)
  n = length(Counties)
  if(n > 1)
  sample = sample(Counties, ceiling(n / 2))
  else sample = Counties
  return(sample)
}))

final1 = final1[, -1]
final2 = final2[, -1]
final3 = final3[, -1]
```

  Now that we already have the county index for the train test, we can now divide the data frame final1 apart. Let's call the test set, `test_1`, `test_2` and `test_3`, and the train set, `train_1`, `train_2` and `train_3`. Also, we call the truth of the train set `trainTruth` and the truth of the test set `testTruth`.
```{r echo = codeInclude}
train_1 = final1[chooseTrain, ]
test_1 = final1[-chooseTrain, ]

train_2 = final2[chooseTrain, ]
test_2 = final2[-chooseTrain, ]

train_3 = final3[chooseTrain, ]
test_3 = final3[-chooseTrain, ]

trainTruth = final[chooseTrain, "change1216"]
testTruth = final[-chooseTrain, "change1216"]
```
  
#### Building the K-NN Predictor

  Next, We want to build a predictor using the K-NN method for several values of the tuning parameter `k`. We can compare the truth with the predictions by calculating error rates and select a proper value for `k` to make the error rates relatively small.
  
  Here, we make different groups of predictions for the test set based on the train set by setting this parameter from 1 to 20. The predictions corresponding to different `k` for each case are put into the matrices, `preds_1`, `preds_2` and `preds_3`.  
```{r echo = codeInclude}
k = 20
preds_1 = matrix(nrow = nrow(test), ncol = k)
preds_2 = matrix(nrow = nrow(test), ncol = k)
preds_3 = matrix(nrow = nrow(test), ncol = k)

library(class)
for (j in 1:k){
  preds_1[, j] = knn(train_1, test_1, trainTruth, j)
  preds_2[, j] = knn(train_2, test_2, trainTruth, j)
  preds_3[, j] = knn(train_3, test_3, trainTruth, j)
} 
```

  Next, we compute the error rates for each `k`.
  
  Notice that the predictions are numbers, 1, 2, 3, 4, which stand for "DD", "DR", "RD" and
"RR" respectively, whereas the type of truth is factor. Thus, we first convert `testTruth` to a numeric vector.
```{r}
testTruth = as.numeric(testTruth)

errorRates_1 = apply(preds_1, 2, function(x){
  sum(x != testTruth)/length(testTruth)
})

errorRates_2 = apply(preds_2, 2, function(x){
  sum(x != testTruth)/length(testTruth)
})

errorRates_3 = apply(preds_3, 2, function(x){
  sum(x != testTruth)/length(testTruth)
})
```

  Plot the error rates for the three ways of choosing features to find out which way is better, or say, which features are more influential on the results. 
```{r echo = codeInclude}
library(ggplot2)
ggplot(data = 
         data.frame(kvalue = 1:k, errorRate = c(errorRates_1, errorRates_2, errorRates_3), choice = rep(c("Choice 1", "Choice 2", "Choice 3"), c(20, 20, 20))))+
  geom_line(aes(kvalue, errorRate, col = choice))+
  scale_x_continuous(name = "kvalue")+
  scale_y_continuous(name = "error rate")+
  ggtitle("Error rates under the three choices")
```
  
  The plot shows that the error rates under the second choice are always smaller than those under the other choices. Therefore, we decide to choose the varibles, "Latitude", "Longitude", "marriedcouples", "familiesWithKids" as our influential features.
  
  By computing the error rates, We can see how many predictions are different from the truth in total. However, considering that the number of the counties that stay voting for the Democratic/Republican party is large and what we care about is the swing counties, it is better to calculate error rates for each classification.  
```{r echo = codeInclude}
errorRates = numeric()
for(i in 1:4){
  errorRates = c(errorRates, apply(preds_2, 2, function(x) {
  sum(x[testTruth == i] != i) / length(x[testTruth == i])
}))
}

ggplot(data = 
         data.frame(kvalue = 1:k, errorRates, class = rep(c("DD", "DR", "RD", "RR"), c(20, 20, 20, 20))))+
  geom_line(aes(x = kvalue, y = errorRates, color = class))+
  scale_x_continuous(name = "k value")+
  scale_y_continuous(name = "error rate")+
  ggtitle("Error rates for each classification")
```

  From the plot, it can be seen that it is proper to set the value of `k` to be 4, since the error rates for each classification are relatively small. 
  
  Moreover, it also shows that the error rates for "DD" and "RR", which means that the voting results do not change, are smaller and those for "DR" and "RD", which means that counties do not vote as they did, are extremely big, even equal to 1.  
  
  We want to find out the reasons and we assume that it is related to percent of white and percent of single mothers. Thus we make a scatter plot and color the points by classifications to see if there are some connections.
```{r echo = codeInclude}
ggplot(data = final[, c("percentWhite", "singleMoms", "change1216")])+
  geom_point(aes(percentWhite, singleMoms, color = change1216), alpha = 0.15, position = 'jitter')+
  scale_x_continuous(name = "percent of white people")+
  scale_y_continuous(name = "percent of single mothers")+
  ggtitle("The relationship between voting changes, percent of white and percent of single mothers")
```

  It shows that counties with lager percent of white people and smaller percent of single mothers tend to change their votes, supporting republican, instead of voting for democratic as the last election. Therefore, we can say that the percent of white people and single mothers might have some influence on the voting changes from democratic to republican.  
  
  In addition, the number of counties which change to vote for democratic is quite small since we can only see a few points in the plot, which may account for the large error rates for "RD". 
  
  What's more, counties with lager percent of white people and smaller percent of single mothers tend to vote for republican, whether they stay voting for republican or they change to vote for republican, which corresponds to what we find in step 2.

  After finishing the KNN predictor, we remove the unnecessary intermediate variables.
```{r echo = codeInclude}
rm(final1, final2, final3, test_1, test_2, test_3, train_1, train_2, train_3, preds_1, preds_2,preds_3, errorRates_1, errorRates_2, errorRates_3)
```

